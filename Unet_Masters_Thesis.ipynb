{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Unet Master's Thesis",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVEX55XaUCRe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82zN0y3WqVgR"
      },
      "source": [
        "## **Dataset loader definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEhvqhl5Gco6"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  \n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from skimage.io import imread\n",
        "import cv2\n",
        "\n",
        "from skimage.color import rgb2gray\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class DataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, split=\"Training + validation\", path_to_data='/content/drive/MyDrive/UNET/Data_new', fold = 0, isval = True):\n",
        "        self.split = split\n",
        "        self.path = path_to_data + '/' + split \n",
        "        self.isval = isval\n",
        "\n",
        "        self.files_img = glob.glob(self.path + '/images/*')\n",
        "        self.files_mask = glob.glob(self.path + '/masks/*')\n",
        "        self.files_img.sort()\n",
        "        self.files_mask.sort()\n",
        "\n",
        "        start_index = fold*200    # for cross-validation fold split\n",
        "        end_index = fold*200 + 200\n",
        "\n",
        "        self.img_filepaths = []  \n",
        "        self.mask_filepaths = []  \n",
        "\n",
        "        if split == \"Training + validation\":  # for dataset augmentation\n",
        "          self.files_img=self.files_img*10\n",
        "          self.files_mask=self.files_mask*10\n",
        "\n",
        "          for i in range(len(self.files_img)):  # separating training and validation fold\n",
        "            if isval:\n",
        "              if start_index <= i and i < end_index:\n",
        "                self.img_filepaths.append(self.files_img[i])\n",
        "                self.mask_filepaths.append(self.files_mask[i])\n",
        "            else:\n",
        "              if start_index > i or i >= end_index:\n",
        "                self.img_filepaths.append(self.files_img[i])\n",
        "                self.mask_filepaths.append(self.files_mask[i])\n",
        "        else:\n",
        "          self.img_filepaths = self.files_img \n",
        "          self.mask_filepaths = self.files_mask\n",
        "      \n",
        "        print(self.img_filepaths)\n",
        "        print(self.mask_filepaths)\n",
        "\n",
        "        self.num_of_imgs = len(self.img_filepaths)\n",
        "        print('Number of images: '+ str(len(self.img_filepaths)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_of_imgs\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = imread(self.img_filepaths[index]).astype('uint8')\n",
        "        mask = imread(self.mask_filepaths[index]).astype('uint8')\n",
        "\n",
        "        img = rgb2gray(img).astype(np.float32)\n",
        "        mask = rgb2gray(mask).astype(np.float32)\n",
        "\n",
        "        img = cv2.resize(img,(512, 512))\n",
        "        mask = cv2.resize(mask,(512, 512))\n",
        "        \n",
        "        mask = mask > 0.5\n",
        "\n",
        "        H = img.shape[0]\n",
        "        W = img.shape[1]\n",
        "\n",
        "        img_size=(round(W/2), round(H/2))\n",
        "\n",
        "        if self.split == \"Training + validation\" and self.isval == False:  # dataset augmentation\n",
        "            img,mask = self.random_crop((H, W),img_size,img,mask)\n",
        "            img,mask = self.random_rotflip(img,mask)\n",
        "\n",
        "        img = TF.to_tensor(img)\n",
        "        mask = TF.to_tensor(mask)\n",
        "\n",
        "        return img,mask\n",
        "\n",
        "\n",
        "\n",
        "    def random_crop(self,in_size,out_size,img,mask):\n",
        "\n",
        "        r = [int(torch.randint(in_size[0]-out_size[0],(1,1)).view(-1).numpy()),int(torch.randint(in_size[1]-out_size[1],(1,1)).view(-1).numpy())]\n",
        "\n",
        "        img_crop = img[r[0]:r[0]+out_size[0],r[1]:r[1]+out_size[1]]\n",
        "        mask_crop = mask[r[0]:r[0]+out_size[0],r[1]:r[1]+out_size[1]]\n",
        "\n",
        "        return img_crop.copy(), mask_crop.copy()\n",
        "\n",
        "\n",
        "\n",
        "    def random_rotflip(self,img,mask):\n",
        "\n",
        "        r = [torch.randint(2,(1,1)).view(-1).numpy(), torch.randint(2,(1,1)).view(-1).numpy(), torch.randint(4,(1,1)).view(-1).numpy()]\n",
        "\n",
        "        if r[0]:\n",
        "            img = np.fliplr(img)\n",
        "            mask = np.fliplr(mask)\n",
        "        if r[1]:\n",
        "            img = np.flipud(img)\n",
        "            mask = np.flipud(mask)\n",
        "\n",
        "        img = np.rot90(img, k = r[2])\n",
        "        mask = np.rot90(mask, k = r[2]) \n",
        "\n",
        "        return img.copy(), mask.copy()\n",
        "    \n",
        "\n",
        "      \n",
        "loader = DataSet(split='Training + validation', fold = 0, isval = False)\n",
        "trainloader = torch.utils.data.DataLoader(loader, batch_size=1, num_workers=4, shuffle=True, pin_memory=True)\n",
        "\n",
        "loader = DataSet(split='Training + validation', fold = 0, isval = True)\n",
        "validloader = torch.utils.data.DataLoader(loader, batch_size=1, num_workers=4, shuffle=True, pin_memory=True)\n",
        "\n",
        "loader = DataSet(split='Testing')\n",
        "testloader = torch.utils.data.DataLoader(loader, batch_size=1, num_workers=4, shuffle=True, pin_memory=True)\n",
        "\n",
        "for it,(data,mask) in enumerate(trainloader):\n",
        "    print('training instance')\n",
        "    plt.imshow(data[0,0,:,:].detach().cpu(), cmap = 'gray')\n",
        "    plt.show()\n",
        "    plt.imshow(mask[0,0,:,:].detach().cpu(), cmap = 'gray')\n",
        "    plt.show()\n",
        "    break\n",
        "   \n",
        "for it,(data,mask) in enumerate(validloader):\n",
        "    print('validation instance')\n",
        "    plt.imshow(data[0,0,:,:].detach().cpu(), cmap = 'gray')\n",
        "    plt.show()\n",
        "    plt.imshow(mask[0,0,:,:].detach().cpu(), cmap = 'gray')\n",
        "    plt.show()\n",
        "    break\n",
        "\n",
        "for it,(data,mask) in enumerate(testloader):\n",
        "    print('testing instance')\n",
        "    plt.imshow(data[0,0,:,:].detach().cpu(), cmap = 'gray')\n",
        "    plt.show()\n",
        "    plt.imshow(mask[0,0,:,:].detach().cpu(), cmap = 'gray')\n",
        "    plt.show()\n",
        "    break"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkWWqH6aqvv2"
      },
      "source": [
        "## **U-Net architecture definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fygVF5iKAAku"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import torch\n",
        "from torch.nn import init\n",
        "\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolution Block \n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(conv_block, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up_conv(nn.Module):\n",
        "    \"\"\"\n",
        "    Up Convolution Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(up_conv, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_ch , out_ch, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Unet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch = 1, out_ch = 1):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "        \n",
        "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(in_ch, filters[0])\n",
        "        self.Conv2 = conv_block(filters[0], filters[1])\n",
        "        self.Conv3 = conv_block(filters[1], filters[2])\n",
        "        self.Conv4 = conv_block(filters[2], filters[3])\n",
        "        self.Conv5 = conv_block(filters[3], filters[4])\n",
        "\n",
        "        self.Up5 = up_conv(filters[4], filters[3])\n",
        "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
        "\n",
        "        self.Up4 = up_conv(filters[3], filters[2])\n",
        "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
        "\n",
        "        self.Up3 = up_conv(filters[2], filters[1])\n",
        "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
        "\n",
        "        self.Up2 = up_conv(filters[1], filters[0])\n",
        "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
        "\n",
        "        self.Conv = nn.Conv2d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        for i, m in enumerate(self.modules()):  # weight and bias initialization\n",
        "          if isinstance(m, nn.Conv2d):\n",
        "            init.xavier_normal_(m.weight)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        e1 = self.Conv1(x)\n",
        "\n",
        "        e2 = self.Maxpool1(e1)\n",
        "        e2 = self.Conv2(e2)\n",
        "\n",
        "        e3 = self.Maxpool2(e2)\n",
        "        e3 = self.Conv3(e3)\n",
        "\n",
        "        e4 = self.Maxpool3(e3)\n",
        "        e4 = self.Conv4(e4)\n",
        "\n",
        "        e5 = self.Maxpool4(e4)\n",
        "        e5 = self.Conv5(e5)\n",
        "\n",
        "\n",
        "        d5 = self.Up5(e5)\n",
        "        d5 = torch.cat((e4, d5), dim=1)\n",
        "        d5 = self.Up_conv5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((e3, d4), dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((e2, d3), dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((e1, d2), dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        out = self.Conv(d2)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6EXAiRKSBC_"
      },
      "source": [
        "net=Unet()\n",
        "net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kldlDN5WrRhn"
      },
      "source": [
        "## **Dice loss definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Fxbs4OrY2V"
      },
      "source": [
        "def dice_loss(X, Y):\n",
        "  \n",
        "    eps = 1.\n",
        "\n",
        "    dice = ((2. * torch.sum(X*Y) + eps) / (torch.sum(X) + torch.sum(Y) + eps) )\n",
        "\n",
        "    return 1 - dice"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruF95EIkrWzP"
      },
      "source": [
        "## **Training + validation loop with cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L1HfpxWnb5A"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  \n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from skimage.io import imread\n",
        "\n",
        "from torch.utils import data\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import random\n",
        "\n",
        "from plotly import graph_objects as go\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "batch = 16\n",
        "epochs = 140\n",
        "\n",
        "for i in range(5):     # five crossvalidation folds\n",
        "\n",
        "    loader = DataSet(split='Training + validation', fold = i, isval = False)\n",
        "    trainloader = torch.utils.data.DataLoader(loader, batch_size=batch, num_workers=2, shuffle=True, pin_memory=True)\n",
        "\n",
        "    loader = DataSet(split='Training + validation', fold = i, isval = True)\n",
        "    validloader= torch.utils.data.DataLoader(loader, batch_size=batch, num_workers=2, shuffle=False, pin_memory=True)\n",
        "\n",
        "    net = Unet().cuda()\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-8)  # AdamW optimizer\n",
        "\n",
        "    sheduler = StepLR(optimizer,step_size=10, gamma=1) # learning rate decay\n",
        "\n",
        "\n",
        "    train_loss = []\n",
        "    validation_loss = []\n",
        "    best_loss = 1    # float for picking model in epoch with the lowest validation loss\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        loss_tmp = []\n",
        "\n",
        "        for k,(data,lbl) in enumerate(trainloader):   # training\n",
        "\n",
        "            data = data.cuda()\n",
        "            lbl = lbl.cuda()\n",
        "\n",
        "            net.train()\n",
        "\n",
        "            output = net(data)\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            loss = dice_loss(lbl,output)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            lbl_num = lbl.detach().cpu()\n",
        "            clas = output.detach().cpu() > 0.5\n",
        "            \n",
        "            loss_tmp.append(loss.detach().cpu())\n",
        "\n",
        "        train_loss.append(np.mean(loss_tmp))\n",
        "\n",
        "  \n",
        "        plt.imshow(data[0,0,:,:].detach().cpu(),'gray')   # progress visualization\n",
        "        plt.title('Training - Original')\n",
        "        plt.show()\n",
        "\n",
        "        plt.imshow(lbl[0,0,:,:].detach().cpu(),'gray')\n",
        "        plt.title('Training - Mask')\n",
        "        plt.show()\n",
        "\n",
        "        plt.imshow(clas[0,0,:,:], 'gray')\n",
        "        plt.title('Training - Prediction')\n",
        "        plt.show()  \n",
        "\n",
        "\n",
        "        loss_tmp = []\n",
        "\n",
        "        for kk,(data,lbl) in enumerate(validloader):  # validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                data = data.cuda()\n",
        "                lbl = lbl.cuda()\n",
        "                net.eval()\n",
        "\n",
        "                output = net(data)\n",
        "                output = torch.sigmoid(output)\n",
        "\n",
        "                loss = dice_loss(lbl,output)\n",
        "                \n",
        "                lbl_num = lbl.detach().cpu()\n",
        "                clas = output.detach().cpu() > 0.5\n",
        "\n",
        "                loss_tmp.append(loss.cpu().detach().numpy())\n",
        "          \n",
        "        validation_loss.append(np.mean(loss_tmp))\n",
        "\n",
        "\n",
        "        if best_loss > validation_loss[epoch]:    # picking model in epoch with the lowest validation loss\n",
        "            best_loss = validation_loss[epoch]\n",
        "            params = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': net.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "                } \n",
        "\n",
        "        sheduler.step()\n",
        "\n",
        "\n",
        "        fig = go.Figure()   # visualizations of learning curves and processed images\n",
        "        fig.update_layout(title='Learning curves',\n",
        "                          yaxis_title = \"Dice loss\",\n",
        "                          xaxis_title = 'Iterations',\n",
        "                          title_font_size=30,\n",
        "                          font_size = 22,\n",
        "                          title_x = 0.5,\n",
        "                          # legend_orientation='h'\n",
        "                          legend_x = 0.79,\n",
        "                          legend_y = 1,)\n",
        "        fig.update_yaxes(title_font_size = 25)\n",
        "        fig.update_xaxes(title_font_size = 25)\n",
        "        fig.add_trace(go.Scatter(x=[a for a in range(epochs)], y=validation_loss,\n",
        "                                mode='lines',\n",
        "                                name='validation loss',\n",
        "                                line_color='red'))\n",
        "        fig.add_trace(go.Scatter(x=[a for a in range(epochs)], y=train_loss,\n",
        "                                mode='lines',\n",
        "                                name='training loss',\n",
        "                                line_color='orange'))\n",
        "        fig.show()\n",
        "\n",
        "        plt.imshow(data[0,0,:,:].detach().cpu().numpy(),'gray')\n",
        "        plt.title('Validation - Original')\n",
        "        plt.show()\n",
        "\n",
        "        plt.imshow(lbl[0,0,:,:].detach().cpu().numpy(),'gray')\n",
        "        plt.title('Validation - Mask')\n",
        "        plt.show()\n",
        "\n",
        "        plt.imshow(clas[0,0,:,:],'gray')\n",
        "        plt.title('Validation - Prediction')\n",
        "        plt.show()  \n",
        "\n",
        "    torch.save(     # saving the best model\n",
        "        params,\n",
        "        '/content/drive/MyDrive/UNET/Training models/Final_crossval_{}.pt'.format(i+1)\n",
        "        )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJNlNH2wrq1r"
      },
      "source": [
        "## **Number of parameters computation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Ru4FqcnKL_"
      },
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "params"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yGyCqHQshNS"
      },
      "source": [
        "## **Installation of Bayessian optimization method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmWapj4wC3hO"
      },
      "source": [
        "!pip install bayesian-optimization"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOlPBApOs39N"
      },
      "source": [
        "## **Bayesian hyperparameter optimization. Function definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mWGenuk-0n0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  \n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from skimage.io import imread\n",
        "\n",
        "from torch.utils import data\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import sklearn.metrics as met\n",
        "import random\n",
        "import gc\n",
        "\n",
        "\n",
        "np.random.seed(0)    # assurance of reproducibility\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "def Bayes_Unet(batch, epochs, lr, weight_decay, lr_multiplier):\n",
        "\n",
        "    assert type(batch) == int\n",
        "    assert type(epochs) == int\n",
        "\n",
        "    loader = DataSet(split='Training + validation', fold = 3, isval = False)\n",
        "    trainloader = torch.utils.data.DataLoader(loader, batch_size=batch, num_workers=0, shuffle=True, pin_memory=True)\n",
        "\n",
        "    loader = DataSet(split='Training + validation', fold = 3, isval = True)\n",
        "    validloader = torch.utils.data.DataLoader(loader, batch_size=batch, num_workers=0, shuffle=False, pin_memory=True)\n",
        "\n",
        "    net = Unet().cuda()\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay) # AdamW optimizer\n",
        "\n",
        "    sheduler = StepLR(optimizer,step_size=10, gamma=lr_multiplier) # learning rate decay\n",
        "\n",
        "\n",
        "    F1_epoch = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        for k,(data,lbl) in enumerate(trainloader): # training\n",
        "\n",
        "            data = data.cuda()\n",
        "            lbl = lbl.cuda()\n",
        "\n",
        "            net.train()\n",
        "\n",
        "            output = net(data)\n",
        "            output = torch.sigmoid(output)\n",
        "\n",
        "            loss = dice_loss(lbl,output)\n",
        "    \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        gc.collect()    # GPU cache emptying\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        F1_batch = []   \n",
        "\n",
        "        for kk,(data,lbl) in enumerate(validloader):  # validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                data = data.cuda()\n",
        "                lbl = lbl.cuda()\n",
        "\n",
        "                net.eval()\n",
        "\n",
        "                output = net(data)\n",
        "                output = torch.sigmoid(output)\n",
        "\n",
        "                loss = dice_loss(lbl,output)\n",
        "                \n",
        "                lbl_num = lbl.detach().cpu()\n",
        "                clas = output.detach().cpu() > 0.5\n",
        "\n",
        "                F1_image = []\n",
        "\n",
        "                for inst in range(len(data)): # metric calcualtion\n",
        "\n",
        "                    cm = met.confusion_matrix(lbl_num[inst,0,:,:].flatten(), clas[inst,0,:,:].flatten())\n",
        "\n",
        "                    TP = cm[1][1]\n",
        "                    FP = cm[0][1]\n",
        "                    FN = cm[1][0]\n",
        "                    TN = cm[0][0]\n",
        "\n",
        "                    F1_image.append((2 * TP) / (2 * TP + FP + FN))        \n",
        "                \n",
        "                F1_batch.append(np.mean(F1_image))\n",
        "\n",
        "        F1_epoch.append(np.mean(F1_batch))\n",
        "\n",
        "        gc.collect()  # GPU cache emptying\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        sheduler.step()\n",
        "\n",
        "    return np.max(F1_epoch)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_VTd8XtT3N"
      },
      "source": [
        "## **Optimization definition + optimization loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxCEmDY5C9Z0"
      },
      "source": [
        "from bayes_opt.logger import JSONLogger\n",
        "from bayes_opt.event import Events\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "def function_to_be_optimized(batch, epochs, lr, weight_decay, lr_multiplier):\n",
        "    batch = int(batch)\n",
        "    epochs = int(epochs)\n",
        "    return Bayes_Unet(batch, epochs, lr, weight_decay, lr_multiplier)\n",
        "\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'batch': (4, 16), 'epochs': (60, 140), 'lr': (1e-6, 1e-2), 'weight_decay': (1e-8, 1e-3), 'lr_multiplier': (0.1, 1)}\n",
        "\n",
        "hyperparams = BayesianOptimization(\n",
        "    f=function_to_be_optimized,\n",
        "    pbounds=pbounds,\n",
        "    verbose=2,\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "logger = JSONLogger(path=\"/content/drive/MyDrive/UNET/Training models/logs.json\")\n",
        "hyperparams.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
        "\n",
        "hyperparams.maximize(\n",
        "    init_points=3,   # 3 random trials\n",
        "    n_iter=30    # 30 Bayesian steps\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnyreehMsMo4"
      },
      "source": [
        "## **Lowest loss + epoch in which this loss was achieved**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kduE8IoJnz9o"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/UNET/Training models/Final_crossval_5.pt'\n",
        "net_params = torch.load(PATH)\n",
        "print(net_params['loss'], net_params['epoch'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3DzGw8Etph2"
      },
      "source": [
        "## **Testing loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYMAW9AlXQIe"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  \n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from skimage.io import imread\n",
        "\n",
        "import sklearn.metrics as met\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "from plotly import graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "loader = DataSet(split='Testing')\n",
        "testloader = torch.utils.data.DataLoader(loader, batch_size=20, num_workers=0, shuffle=False, pin_memory=True)\n",
        "\n",
        "# PATH = '/content/drive/MyDrive/UNET/Training models/Best_model_fold_1.pt'\n",
        "# PATH = '/content/drive/MyDrive/UNET/Training models/No_augmentation_1.pt'\n",
        "# PATH = '/content/drive/MyDrive/UNET/Training models/No_skip_connections_1.pt'\n",
        "PATH = '/content/drive/MyDrive/UNET/Training models/Final_crossval_5.pt'\n",
        "\n",
        "net_params = torch.load(PATH)   # loading of trained model\n",
        "net = Unet().cuda()\n",
        "net.load_state_dict(net_params['model_state_dict'])\n",
        "\n",
        "\n",
        "for kk,(data,lbl) in enumerate(testloader):   # testing\n",
        "    with torch.no_grad():\n",
        "        data = data.cuda()\n",
        "        lbl = lbl.cuda()\n",
        "        net.eval()\n",
        "\n",
        "        output = net(data)\n",
        "\n",
        "        output = torch.sigmoid(output)\n",
        "\n",
        "        lbl_num = lbl.detach().cpu()\n",
        "        clas = output.detach().cpu() > 0.5\n",
        "\n",
        "\n",
        "        Recall = []\n",
        "        Precision = []\n",
        "        F1 = []\n",
        "\n",
        "        for inst in range(len(data)):\n",
        "\n",
        "            cm = met.confusion_matrix(lbl_num[inst,0,:,:].flatten(), clas[inst,0,:,:].flatten())\n",
        "\n",
        "            TP = cm[1][1]\n",
        "            FP = cm[0][1]\n",
        "            FN = cm[1][0]\n",
        "            TN = cm[0][0]\n",
        "\n",
        "            Recall.append(TP / (TP + FN))\n",
        "            Precision.append(TP / (TP + FP))\n",
        "            F1.append((2 * TP) / (2 * TP + FP + FN))        \n",
        "\n",
        "            plt.figure(figsize = [7,7])        # viualization + final image saving \n",
        "            plt.imshow(data.detach().cpu()[inst,0,:,:] ,'gray')\n",
        "            plt.title('Image')\n",
        "            plt.show()\n",
        "            # filename = '/content/drive/MyDrive/UNET/Output_images/Final/Orig_{}.png'.format(inst+1)\n",
        "            # cv2.imwrite(filename, np.uint8(data.detach().cpu()[inst,0,:,:]))\n",
        "\n",
        "            plt.figure(figsize = [7,7])\n",
        "            plt.imshow(lbl_num[inst,0,:,:] ,'gray')\n",
        "            plt.title('Label')\n",
        "            plt.show()\n",
        "            # filename = '/content/drive/MyDrive/UNET/Output_images/Final/Mask_{}.png'.format(inst+1)\n",
        "            # cv2.imwrite(filename, np.uint8(lbl_num[inst,0,:,:])*255)\n",
        "\n",
        "            plt.figure(figsize = [7,7])\n",
        "            plt.imshow(clas[inst,0,:,:], 'gray')\n",
        "            plt.title('Output')\n",
        "            plt.show()\n",
        "            # filename = '/content/drive/MyDrive/UNET/Output_images/Final/Output_{}.png'.format(inst+1)\n",
        "            # cv2.imwrite(filename, np.uint8(clas[inst,0,:,:])*255)\n",
        "\n",
        "            print(inst)\n",
        "            print('Recall: '+str(round(Recall[inst],3))+ ', Precision: '+str(round(Precision[inst],3))+ ', F1: ' +str(round(F1[inst],3))) # individual metric printing\n",
        "            \n",
        "        Recall_final = np.mean(Recall)\n",
        "        Precision_final = np.mean(Precision)\n",
        "        F1_final = np.mean(F1)\n",
        "\n",
        "    \n",
        "plt.figure(figsize = [10,10])   # visualization of metrics\n",
        "plt.plot(F1, label = 'F1')\n",
        "plt.plot(Precision, label = 'Precision')\n",
        "plt.plot(Recall, label = 'Recall')\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.title('Evaluation metrics')\n",
        "plt.xlabel('Images')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis = 'y')\n",
        "plt.show()\n",
        "\n",
        "fig = go.Figure()   # boxplot of model\n",
        "fig.update_layout(yaxis_title = \"Metric range\", title_font_size=30, font_size = 22, title_x = 0.5)\n",
        "fig.update_yaxes(title_font_size = 25)\n",
        "fig.add_trace(go.Box(y=Recall, name='Recall', fillcolor = 'rgba(255,0,0,0.5)', marker_color = 'rgba(255,0,0,1)'))\n",
        "fig.add_trace(go.Box(y=Precision, name='Precision'))\n",
        "fig.add_trace(go.Box(y=F1, name='Dice score', fillcolor = 'rgba(255,50,50,0.5)', marker_color = 'rgba(255,0,100,1)'))\n",
        "fig.show()\n",
        "\n",
        "print('') # printing final information of the model\n",
        "print('Epoch: '+str(net_params['epoch'])+ ', Loss: '+str(net_params['loss']))\n",
        "print('Recall: '+str(round(Recall_final,3))+ ', Precision: '+str(round(Precision_final,3))+ ', F1: ' +str(round(F1_final,3)))"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}